{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UhGu2p6joEaK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4f2uJZAoocY"
      },
      "outputs": [],
      "source": [
        "# Gemini API key 都放進來\n",
        "key_list = [\n",
        "            \n",
        "            ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIsk27urpWGc"
      },
      "outputs": [],
      "source": [
        "# 定義來源與輸出資料夾\n",
        "SOURCE_DIR = \"\"\n",
        "OUTPUT_DIR = \"\"\n",
        "OUTPUT_DIR_GEN = \"\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR_GEN, exist_ok=True)\n",
        "\n",
        "# 目前目錄生產到的檔案編號(從0開始)\n",
        "# AI generate 會用到而已\n",
        "file_num = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 過濾掉劣質data以及刪除粗體的*符號\n",
        "\n",
        "def remove_asterisks(input_string):\n",
        "    return input_string.replace('*', '')\n",
        "\n",
        "def check_string(s):\n",
        "    letter_count = sum(('A' <= c <= 'Z') or ('a' <= c <= 'z') for c in s)\n",
        "    digit_count = sum(c.isdigit() for c in s)\n",
        "\n",
        "    if letter_count > 500 or digit_count > 300:\n",
        "        return False\n",
        "    else:\n",
        "        return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9Ljv0105s62A",
        "outputId": "314d506e-6f2e-4a46-b0b1-932c43b4aa8a"
      },
      "outputs": [],
      "source": [
        "# AI paraphrase\n",
        "# 根據論文原文抓出來的片段進行改寫\n",
        "\n",
        "# 正則表達式匹配 \"數字.txt\" 格式的檔案\n",
        "pattern = re.compile(r\"^\\d+\\.txt$\")\n",
        "GENAI_API_KEY = key_list[0]\n",
        "key_index = 0\n",
        "\n",
        "# 遍歷來源資料夾的所有檔案\n",
        "for filename in os.listdir(SOURCE_DIR):\n",
        "    if pattern.match(filename):\n",
        "        file_path = os.path.join(SOURCE_DIR, filename)\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            content = f.read()\n",
        "\n",
        "        try:\n",
        "            # 呼叫 Gemini API 進行改寫\n",
        "            genai.configure(api_key=GENAI_API_KEY)\n",
        "            model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
        "            response = model.generate_content(\"請改寫以下內容，不要回傳改寫以外的內容:\" + content)\n",
        "            rewritten_text = response.text\n",
        "        except:\n",
        "            key_index = key_index + 1\n",
        "            if key_index >= len(key_list):\n",
        "                break\n",
        "            GENAI_API_KEY = key_list[key_index]\n",
        "\n",
        "            # 呼叫 Gemini API 進行改寫\n",
        "            genai.configure(api_key=GENAI_API_KEY)\n",
        "            model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
        "            response = model.generate_content(\"請改寫以下內容，不要回傳改寫以外的內容:\" + content)\n",
        "            rewritten_text = response.text\n",
        "\n",
        "        # 儲存到 paraphrase 資料夾\n",
        "        output_path = os.path.join(OUTPUT_DIR, filename)\n",
        "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(rewritten_text)\n",
        "\n",
        "        os.remove(file_path)\n",
        "\n",
        "        print(f\"已處理: {filename} -> {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 根據 extract.py 產生的 index.txt ，將同一篇論文的前後文編號存到一組 list 裡面\n",
        "\n",
        "INDEX_FILE = os.path.join(SOURCE_DIR, \"index.txt\")\n",
        "\n",
        "def parse_index_file(filepath):\n",
        "    result = []\n",
        "\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"{filepath} 不存在。\")\n",
        "        return result\n",
        "\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            try:\n",
        "                parts = line.split(':')\n",
        "                if len(parts) != 2:\n",
        "                    continue\n",
        "\n",
        "                txt_range = parts[1].strip()\n",
        "                start_txt, end_txt = txt_range.split('-')\n",
        "                start_num = int(start_txt.strip().replace('.txt', ''))\n",
        "                end_num = int(end_txt.strip().replace('.txt', ''))\n",
        "\n",
        "                if start_num < end_num:\n",
        "                    result.append([start_num, end_num])\n",
        "            except Exception as e:\n",
        "                print(f\"解析失敗: {line}，錯誤: {e}\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "data = parse_index_file(INDEX_FILE)\n",
        "print(\"結果陣列：\")\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "已處理: 263.txt -> D:\\py\\plag\\paper\\generate\\ncu_2019\\263.txt\n",
            "已處理: 264.txt -> D:\\py\\plag\\paper\\generate\\ncu_2019\\264.txt\n",
            "已處理: 265.txt -> D:\\py\\plag\\paper\\generate\\ncu_2019\\265.txt\n",
            "已處理: 266.txt -> D:\\py\\plag\\paper\\generate\\ncu_2019\\266.txt\n",
            "已處理: 267.txt -> D:\\py\\plag\\paper\\generate\\ncu_2019\\267.txt\n",
            "已處理: 268.txt -> D:\\py\\plag\\paper\\generate\\ncu_2019\\268.txt\n",
            "已處理: 269.txt -> D:\\py\\plag\\paper\\generate\\ncu_2019\\269.txt\n",
            "已處理: 270.txt -> D:\\py\\plag\\paper\\generate\\ncu_2019\\270.txt\n",
            "已處理: 271.txt -> D:\\py\\plag\\paper\\generate\\ncu_2019\\271.txt\n",
            "已處理: 272.txt -> D:\\py\\plag\\paper\\generate\\ncu_2019\\272.txt\n",
            "已處理: 273.txt -> D:\\py\\plag\\paper\\generate\\ncu_2019\\273.txt\n",
            "已處理: 274.txt -> D:\\py\\plag\\paper\\generate\\ncu_2019\\274.txt\n",
            "已處理: 275.txt -> D:\\py\\plag\\paper\\generate\\ncu_2019\\275.txt\n",
            "已處理: 276.txt -> D:\\py\\plag\\paper\\generate\\ncu_2019\\276.txt\n",
            "已處理: 277.txt -> D:\\py\\plag\\paper\\generate\\ncu_2019\\277.txt\n",
            "已處理: 278.txt -> D:\\py\\plag\\paper\\generate\\ncu_2019\\278.txt\n",
            "已處理: 279.txt -> D:\\py\\plag\\paper\\generate\\ncu_2019\\279.txt\n",
            "已處理: 280.txt -> D:\\py\\plag\\paper\\generate\\ncu_2019\\280.txt\n",
            "已處理: 281.txt -> D:\\py\\plag\\paper\\generate\\ncu_2019\\281.txt\n",
            "已處理: 282.txt -> D:\\py\\plag\\paper\\generate\\ncu_2019\\282.txt\n"
          ]
        }
      ],
      "source": [
        "# AI generation\n",
        "# 根據免篇論文的前後文，讓Gemini生成中間段落\n",
        "\n",
        "GENAI_API_KEY = key_list[0]\n",
        "key_index = 0\n",
        "\n",
        "# 遍歷來源資料夾的所有檔案\n",
        "for pair in data:\n",
        "    file_path_start = os.path.join(SOURCE_DIR, str(pair[0]) + \".txt\")\n",
        "    file_path_end = os.path.join(SOURCE_DIR, str(pair[1]) + \".txt\")\n",
        "    \n",
        "    with open(file_path_start, \"r\", encoding=\"utf-8\") as f:\n",
        "        content_start = f.read()\n",
        "\n",
        "    with open(file_path_end, \"r\", encoding=\"utf-8\") as f:\n",
        "        content_end = f.read()\n",
        "        \n",
        "    try:\n",
        "        # 呼叫 Gemini API 進行生成\n",
        "        genai.configure(api_key=GENAI_API_KEY)\n",
        "        model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
        "        response = model.generate_content(\"以下兩段文章為同一篇論文中前面章節與後面章節內容抓出來的小片段，請生成一個2000字以上類似的小片段，內容為你認為這兩段文章中間應該要有的文字，不要回傳標題以及與內容無關的文字，也不要使用粗體排版或標標題。\\n文章一:\\n\" + content_start + \"\\n文章二:\\n\" + content_end)\n",
        "        rewritten_text = response.text\n",
        "        if rewritten_text.count('*') > 4:\n",
        "            rewritten_text = remove_asterisks(rewritten_text)\n",
        "    except:\n",
        "        key_index = key_index + 1\n",
        "        if key_index >= len(key_list):\n",
        "            break\n",
        "        GENAI_API_KEY = key_list[key_index]\n",
        "\n",
        "        # 呼叫 Gemini API 進行生成\n",
        "        genai.configure(api_key=GENAI_API_KEY)\n",
        "        model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
        "        response = model.generate_content(\"以下兩段文章為同一篇論文中前面章節與後面章節內容抓出來的小片段，請生成一個2000字以上類似的小片段，內容為你認為這兩段文章中間應該要有的文字，不要回傳標題以及與內容無關的文字，也不要使用粗體排版或標標題，。\" + \"\\n文章一:\\n\" + content_start + \"\\n文章二:\\n\" + content_end)\n",
        "        rewritten_text = response.text\n",
        "        if rewritten_text.count('*') > 4:\n",
        "            rewritten_text = remove_asterisks(rewritten_text)\n",
        "\n",
        "    # 儲存到 generate 資料夾\n",
        "    if check_string(rewritten_text):\n",
        "        filename_num = filename_num + 1\n",
        "        filename = str(filename_num) + \".txt\"\n",
        "        output_path = os.path.join(OUTPUT_DIR, filename)\n",
        "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(rewritten_text)\n",
        "\n",
        "        print(f\"已處理: {filename} -> {output_path}\")    "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "paper_crawler",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
